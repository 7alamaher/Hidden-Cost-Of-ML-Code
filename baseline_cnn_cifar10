# baseline_cnn_cifar10.py
# Clean baseline: CIFAR-10 + simple 4-conv-layer CNN

import os
import time
import random
import numpy as np
import tensorflow as tf


# 1) Make runs reproducible
# We want Python , NumPy and TensorFlow to behave as consistently as possinle accross runs 
SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED) # Mean use the same hasing behavior every time 
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)


# 2) Load and prep CIFAR-10
# CIFAR-10 images are x-train: 32x32 pixels with 3 color channels RGB, y-train: labels are 0..9
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Convert pixels from 0..255 into 0..1 (helps training)
x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32") / 255.0

# Turn labels into shape (N,) instead of (N,1) so Keras can use them cleanly with sparse_categorical_crossentropy loss
y_train = y_train.squeeze()
y_test  = y_test.squeeze()


# 3) Build a simple 4-layer CNN

def build_cnn():
    # A CNN tries to recognize what is inside an image by learning patterns in the pixel data.
    # create a "machine' that 
    # a) takes an image (32*32*03) 
    # b) learns patterns (small patterns -> bigger patterns -> objects) 
    # c) outputs 10 number (one per class) 
    # d) the biggest number = predicted class   
    
    inputs = tf.keras.Input(shape=(32, 32, 3))

    # Conv2D learns patterns in images by applying filters (like edge detectors) across the image.
    #pooling means shrink the image size , Keep the strongest features and reduce the number of parameters, which helps focus on the most important patterns and speeds up training.
    x = tf.keras.layers.Conv2D(32, 3, padding="same", activation="relu")(inputs)
    x = tf.keras.layers.MaxPool2D()(x)

    x = tf.keras.layers.Conv2D(64, 3, padding="same", activation="relu")(x)
    x = tf.keras.layers.MaxPool2D()(x)

    x = tf.keras.layers.Conv2D(128, 3, padding="same", activation="relu")(x)
    x = tf.keras.layers.MaxPool2D()(x)

    x = tf.keras.layers.Conv2D(128, 3, padding="same", activation="relu")(x)

    # Flatten turns the 3D feature maps  "image grid "into a 1D vector, which can (Dense) layers for classification.
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(128, activation="relu")(x)
    outputs = tf.keras.layers.Dense(10, activation="softmax")(x) # 10 outputs because CIFAR-10 has 10 classes.

    model = tf.keras.Model(inputs, outputs)
    return model


model = build_cnn()

# 4) Compile (training settings)
LEARNING_RATE = 1e-3
BATCH_SIZE = 128
EPOCHS = 3  # keep small for testing; can increase later

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"],
)

print(model.summary())


# 5) Train (measure fit time only)
train_start = time.perf_counter()

history = model.fit(
    x_train, y_train,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_split=0.1,
    verbose=2
)

train_end = time.perf_counter()
train_seconds = train_end - train_start



# 6) Evaluate (measure evaluation time only)
eval_start = time.perf_counter()

test_loss, test_acc = model.evaluate(
    x_test, y_test,
    batch_size=BATCH_SIZE,
    verbose=0
)

eval_end = time.perf_counter()
eval_seconds = eval_end - eval_start


# 7) Print baseline results
print("\nCLEAN  CNN BASELINE RESULTS (CIFAR-10): ")
print(f"Train time (fit only): {train_seconds:.3f} seconds")
print(f"Eval time (evaluate only): {eval_seconds:.3f} seconds")
print(f"Test accuracy: {test_acc:.4f}")
print(f"Test loss: {test_loss:.4f}")
